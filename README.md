# Machine learning paper summaries
Summaries of machine learning papers, mostly on NLU / NLP / Deep learning

## To read

- Generating Long Sequences with Sparse Transformers (2019) [[Paper](https://arxiv.org/abs/1904.10509)]
    - Rewon Child, Scott Gray, Alec Radford, Ilya Sutskever

## 2020  

- Specializing Word Embeddings (for Parsing) by Information Bottleneck (2019) [[Summary](./summaries/specializing-word-embeddings-vib.pdf)] [[Paper](https://arxiv.org/abs/1910.00163)]
    - Xiang Lisa Li, Jason Eisner

- Deep Neural Network Compression with Single and Multiple Level Quantization (2018) [[Summary](./summaries/dnn-compression-slq-mlq.pdf)] [[Paper](https://arxiv.org/abs/1803.03289)] 
    - Yuhui Xu, Yongzhuang Wang, Aojun Zhou, Weiyao Lin, Hongkai Xiong

- SLIDE : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems (2019) [[Summary](./summaries/slide-defense.pdf)] [[Paper](https://arxiv.org/abs/1903.03129)]
    - Beidi Chen, Tharun Medini, James Farwell, Sameh Gobriel, Charlie Tai, Anshumali Shrivastava

- Learning Adversarial Networks for Semi-Supervised Text Classification via Policy Gradient (2018) [[Summary](./summaries/adversarial-networks-policy-gradient.pdf)] [[Paper](https://dl.acm.org/doi/10.1145/3219819.3219956)]
    -  Yan Li, Jieping Ye

- Tensor Networks in a Nutshell (2017) [[Summary](./summaries/tensor-network.pdf)] [[Paper](https://arxiv.org/abs/1708.00006)]
    - Jacob Biamonte, Ville Bergholm

- Zero-shot Knowledge Transfer via Adversarial Belief Matching (2019) [[Summary](./summaries/zero-shot-knowledge-transfer.pdf)][[Paper](https://arxiv.org/abs/1905.09768)]
    - Paul Micaelli, Amos Storkey

- Patient Knowledge Distillation for BERT Model Compression (2019) [[Summary](./summaries/patient-knowledge-distillation.pdf)] [[Paper](https://arxiv.org/abs/1908.09355)]
    - Siqi Sun, Yu Cheng, Zhe Gan, Jingjing Liu

- Reformer: The Efficient Transformer (2020) [[Review](./summaries/reformer-the-efficient-transformer.pdf)][[Paper](https://arxiv.org/abs/2001.04451)]
    - Nikita Kitaev, Łukasz Kaiser, Anselm Levskaya

- Legendre Memory Units: Continuous-Time
Representation in Recurrent Neural Networks (2019) [[Summary](./summaries/legendre-memory-units.pdf)][[Paper](https://papers.nips.cc/paper/9689-legendre-memory-units-continuous-time-representation-in-recurrent-neural-networks.pdf)]
    - Aaron R. Voelker, Ivana Kajic, Chris Eliasmith

## 2019 

- Input-Cell Attention Reduces Vanishing Saliency of Recurrent Neural Networks (2019) [[Review](./summaries/input-cell-attention.pdf)][[Paper](https://arxiv.org/abs/1910.12370)]
    - Aya Abdelsalam Ismail, Mohamed Gunady, Luiz Pessoa, Héctor Corrada Bravo, Soheil Feizi

- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (2019) [[Summary](./summaries/text-to-text-transfer-transformer.pdf)][[Paper](https://arxiv.org/abs/1910.10683)]
    - Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu

- Single Headed Attention RNN: Stop Thinking With Your Head (2019) [[Summary](./summaries/single-headed-attention-rnn.pdf)] [[Paper](https://arxiv.org/abs/1911.11423)]
    - Stephen Merity

- Attention is all you need (2019) [[Summary](./summaries/attention-is-all-you-need.pdf)][[Paper](https://arxiv.org/abs/1706.03762)]
    - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin
